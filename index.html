<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Jingxiang Sun">
  <meta name="description" content="Jingxiang Sun's Homepage">
  <meta name="keywords" content="Jingxiang Sun,孙景翔,homepage,主页,PhD,computer vision,Tsinghua,3D reconstruction,image generation,Neural rendering, Digital avatar>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jingxiang Sun (孙景翔)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jingxiang Sun (孙景翔)</name>
              </p>
              <p style="text-align:center">
                Email: sunjingxiang_stark[at]126.com &nbsp; &nbsp;&nbsp; <a href="https://scholar.google.com/citations?hl=en&user=-b1UAp0AAAAJ">Google Scholar</a> &nbsp; &nbsp;&nbsp;<a href="https://github.com/MrTornado24">Github</a> &nbsp; &nbsp;&nbsp;<a href="https://twitter.com/jingxiangsun42">Twitter</a> &nbsp; &nbsp;&nbsp;<a href="https://www.youtube.com/channel/UCtDlyN3TTh6zRMgo23lUtgw">Youtube</a>
              </p>
              <p>I am a second-year PhD student of <a href="https://www.au.tsinghua.edu.cn/">Department of Automation, Tsinghua University</a> in fall, 2022 and under the supervision of <a href="https://liuyebin.com/">Prof. Yebin Liu</a>. 
                Prior to Tsinghua University I obtained M.S. from <a href="https://ece.illinois.edu/">Department of ECE, University of Illinois at Urbana-Champaign</a>. 
                My research focuses on neural rendering, digital avatar and 3D generation.
              </p>
              <p>
                I am very fortunate to have spent summer 2021 at <a href="https://ai.tencent.com/ailab/en/index">Tencent AI Lab</a> with <a href="https://xuanwangvc.github.io/">Xuan Wang</a> and <a href="https://juewang725.github.io/">Jue Wang</a>;
                fall 2020 at <a href="http://www.cad.zju.edu.cn/">State Key Laboratory of CAD&CG</a> at <a href="http://www.zju.edu.cn/english/">Zhejiang University</a> with <a href="https://xzhou.me/">Xiaowei Zhou</a>.
              </p>
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/sunjingxiang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/sunjingxiang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              2024-10:  We introduce DreamCraft3D++, a new technique for high-quality 3D content generation!<br>
              2024-08:  One paper is accepted by SIGGRAPH ASIA 2024!<br>
              2024-03:  One paper is accepted by SIGGRAPH 2024!<br>
              2024-02:  Two papers are accepted by CVPR 2024!<br>
              2024-02:  Start my internship at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research </a> with <a href="https://luminohope.org/">Koki Nagano </a> and <a href="https://research.nvidia.com/person/shalini-de-mello">Shalini De Mello</a>.<br>
              2024-01:  <font color="#FF0000"><strong>DreamCraft3D</strong> is accepted at ICLR 2024. See you in Vienna!<br></font>
              2023-09:  <strong>HAvatar</strong> is accepted by ACM TOG 2023.<br>
              2023-03:  <strong>StyleAvatar</strong> is accepted by ACM SIGGRAPH 2023. <br>
              2023-03:  <strong>Next3D</strong> is selected as CVPR <strong><font color="#FF0000">Highlight</font></strong> papers (10% of accepted papers, 2.5% of submissions). <br>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='DreamCraft3D_plus/demo_dc3d++.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>DreamCraft3D++: Efficient Hierarchical 3D Generation with Multi-Plane Reconstruction Model</papertitle>
              <br>
              <strong>Jingxiang Sun</strong>, Cheng Peng, Ruizhi Shao, Yuan-Chen Guo, Xiaochen Zhao, Yangguang Li, Yanpei Cao, Bo Zhang, Yebin Liu
              <br>
              <em>arXiv</em>, 2024
              <br>
              <a href="https://dreamcraft3dplus.github.io/">[Project]</a>
              <a href="https://arxiv.org/abs/2410.12928">[PDF]</a>
	            <!-- <a href="https://github.com/deepseek-ai/DreamCraft3D">[Code]</a> -->
              <a href="images/dreamcraft3d_plus.txt">[BibTeX]</a>
              <br>
              <p> We present DreamCraft3D++, an extension of DreamCraft3D that enables efficient high-quality generation of complex 3D assets in 10 minutes. </p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/teaser_human4dit.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Human4DiT: 360-degree Human Video Generation with 4D Diffusion Transformer</papertitle>
              <br>
             Ruizhi Shao*, Youxin Pang*, Zerong Zheng, <strong>Jingxiang Sun</strong>, Yebin Liu
              <br>
              ACM Transactions on Graphics (SIGGRAPH Asia 2024)
              <br>
              <a href="https://human4dit.github.io/">[Project]</a>
              <a href="https://arxiv.org/abs/2405.17405">[PDF]</a>
	            <!-- <a href="https://github.com/deepseek-ai/DreamCraft3D">[Code]</a> -->
              <a href="images/human4dit.txt">[BibTeX]</a>
              <br>
              <p> Given a reference image, SMPL sequences and camera parameters, our method is capable of generating free-view dynamic human videos. </p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/dreamcraft3d.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior</papertitle>
              <br>
              <strong>Jingxiang Sun</strong>, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, Yebin Liu
              <br>
              <em>2024 International Conference on Learning Representations</em>, ICLR 2024
              <br>
              <a href="DreamCraft3D/">[Project]</a>
              <a href="https://arxiv.org/abs/2310.16818">[PDF]</a>
	            <a href="https://github.com/deepseek-ai/DreamCraft3D">[Code]</a>
              <a href="images/dreamcraft3d.txt">[BibTeX]</a>
              <br>
              <p> We present DreamCraft3D, a hierarchical 3D content generation method that produces high-fidelity and coherent 3D object. </p>
          </td>
      </tr>


      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/invertavatar_00.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars</papertitle>
              <br>
              Xiaochen Zhao*, <strong>Jingxiang Sun</strong>*, Lizhen Wang, Jinli Suo, Yebin Liu (* equal contribution)
              <br>
              ACM SIGGRAPH 2024 (conditionally accepted)
              <br>
              <a href="https://xchenz.github.io/invertavatar_page/">[Project]</a>
              <a href="https://arxiv.org/abs/2312.02222">[PDF]</a>
              <a href="images/invertavatar.txt">[BibTeX]</a>
              <br>
              <p> We present the <em>Incremental 3D GAN Inversion</em>, which efficiently reconstructs photorealistic 3D facial avatars in under 1s, using single or multiple source images. </p>
          </td>
      </tr>


      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/control4d.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor</papertitle>
              <br>
              Ruizhi Shao, <strong>Jingxiang Sun</strong>, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, Yebin Liu
              <br>
              <em>2024 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2024
              <br>
              <a href="https://control4darxiv.github.io/">[Project]</a>
              <a href="https://arxiv.org/abs/2305.20082">[PDF]</a>
	            <a href="https://github.com/threestudio-project/threestudio">[Code]</a>
              <a href="images/control4d.txt">[BibTeX]</a>
              <br>
              <p> We propose Control4D, an approach to high-fidelity and spatiotemporal-consistent 4D portrait editing with only text instructions. </p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/ram_avatar.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>RAM-Avatar: Real-time Photo-Realistic Avatar from Monocular Videos with Full-body Control</papertitle>
              <br>
              Xiang Deng, Zerong Zheng, Yuxiang Zhang, <strong>Jingxiang Sun</strong>, Chao Xu, XiaoDong Yang, Lizhen Wang, Yebin Liu
              <br>
              <em>2024 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2024
              <br>
              <a href="https://github.com/Xiang-Deng00/RAM-Avatar/">[Project]</a>
              <a href="https://cloud.tsinghua.edu.cn/f/6b7a88c3b4ac43b0b506/?dl=1">[PDF]</a>
	            <a href="https://github.com/Xiang-Deng00/RAM-Avatar/">[Code]</a>
              <a href="images/control4d.txt">[BibTeX]</a>
              <br>
              <p> We present RAM-Avatar, a real-time photo-realistic human avatar learning method based on monocular videos, which not only
                achieves high-fidelity rendering with full-body control including the face and hands but also supports real-time animation. </p>
          </td>
      </tr>


      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/vectortalker.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>VectorTalker: SVG Talking Face Generation with Progressive Vectorisation</papertitle>
              <br>
              Hao Hu, Xuan Wang, <strong>Jingxiang Sun</strong>, Yanbo Fan, Yu Guo, Caigui Jiang
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2312.11568">[Project]</a>
              <a href="https://arxiv.org/abs/2312.11568">[PDF]</a>
              <a href="images/vectortalker.txt">[BibTeX]</a>
              <br>
              <p> We present VectorTalker, a novel method for creating high-fidelity, audio-driven talking heads using scalable vector graphics, effective for various image styles. </p>
          </td>
      </tr>
        
          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/havatar.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>HAvatar: High-fidelity Head Avatar
                via Facial Model ConditionedNeural Radiance Field</papertitle>
              <br>
              Xiaochen Zhao, lizhen Wang, <strong>Jingxiang Sun</strong>, Hongwen Zhang, Jinli Suo, Yebin Liu
              <br>
              ACM Transactions on Graphics (ACM TOG 2023)
              <br>
              <a href="http://www.liuyebin.com/havatar/">[Project]</a>
              <a href="http://www.liuyebin.com/havatar/assets/HAvatar_cameraReady.pdf">[PDF]</a>
              <a href="https://github.com/XChenZ/havatar">[Code]</a>
              <a href="images/havatar.txt">[BibTeX]</a>
              <br>
              <p> We introduce the Facial Model Conditioned Neural Radiance Field, a hybrid 3D representation method that merges NeRF's expressiveness with parametric template data, enabling topological flexibility through synthetic-renderings-based conditioning. </p>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
            <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                        <source src='images/styleavatar.mp4'>
            </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>StyleAvatar: Real-time Photo-realistic Portrait Avatar
                from a Single Video</papertitle>
              <br>
              Lizhen Wang, Xiaochen Zhao, <strong>Jingxiang Sun</strong>, Yuxiang Zhang, Hongwen Zhang, Tao Yu, Yebin Liu
              <br>
              ACM SIGGRAPH 2023
              <br>
              <a href="https://www.liuyebin.com/styleavatar/styleavatar.html">[Project]</a>
              <a href="https://arxiv.org/abs/2305.00942">[PDF]</a>
	      <a href="https://github.com/LizhenWangT/StyleAvatar">[Code]</a>
              <a href="images/styleavatar.txt">[BibTeX]</a>
              <br>
              <p> We propose StyleAvatar, a real-time photo-realistic portrait avatar reconstruction method using StyleGAN-based networks, which can generate high-fidelity portrait avatars with faithful expression control.  </p>
          </td>
      </tr>

          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
              <source src='images/next3d_teaser.mp4'>
  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</papertitle>
              <br>
              <strong>Jingxiang Sun</strong>, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, Yebin Liu
              <br>
              <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023,
		<strong><font color="#FF0000">Highlight</font></strong>
              <br>
              <a href="https://mrtornado24.github.io/Next3D/">[Project]</a>
              <a href="https://arxiv.org/pdf/2211.11208.pdf">[PDF]</a>
	      <a href="https://github.com/MrTornado24/Next3D">[Code]</a>
              <a href="images/next3d.txt">[BibTeX]</a>
              <br>
              <p> We propose a novel 3D GAN framework for unsupervised learning of generative, high-quality and 3D-consistent facial avatars from unstructured 2D images. To achieve both deformation accuracy and topological flexibility, we present a 3D representation called Generative Texture-Rasterized Tri-planes. </p>
          </td>
      </tr>


      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\hffa.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors</papertitle>
              <br>
              Yunpeng Bai, Yanbo Fan, Xuan Wang, Yong Zhang, <strong>Jingxiang Sun</strong>, Chun Yuan, Ying Shan
              <br>
              <em>2023 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2023
              <br>
              <a href="https://arxiv.org/pdf/2211.15064.pdf">[Project]</a>
              <a href="https://arxiv.org/pdf/2211.15064.pdf">[PDF]</a>
              <a href="https://arxiv.org/pdf/2211.15064.pdf">[Code]</a>
      <a href="images/high.txt">[BibTeX]</a>
              <br>
              <p>We propose an efficient method to construct the personalized generative prior based on a small set of facial images of a given individual. After learning, it allows for photo-realistic rendering with novel views and the face reenactment can be realized by performing navigation in the latent space. Our proposed method is applicable for different driven signals, including RGB images, 3DMM coefficients, and audios. </p>
          </td>
      </tr>


        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='images\diffustereo.jpg' style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td> 
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras</papertitle>
              <br>
              Ruizhi Shao, Zerong Zheng, Hongwen Zhang, <strong>Jingxiang Sun</strong>, Yebin Liu
              <br>
              <em>2022 IEEE European Conference on Computer Vision</em>, ECCV 2022,
	      <strong><font color="#FF0000">Oral Presentation</font></strong>
              <br>
              <a href="http://www.liuyebin.com/diffustereo/diffustereo.html">[Project]</a>
              <a href="https://arxiv.org/pdf/2207.08000.pdf">[PDF]</a>
              <a href="https://github.com/DSaurus/DiffuStereo">[Code]</a>
      <a href="images/diffustereo.txt">[BibTeX]</a>
              <br>
              <p>We propose DiffuStereo, a novel system using only sparse cameras (8 in this work) for high-quality 3D human reconstruction. At its core is a novel diffusion-based stereo module, which introduces diffusion models, a type of powerful generative models, into the iterative stereo matching network. </p>
          </td>
      </tr>
        
          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
              <source src='images/teaser_ide_3d.mp4'>
  </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis</papertitle>
              <br>
              <strong>Jingxiang Sun</strong>, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, Yebin Liu
              <br>
              ACM Transactions on Graphics (SIGGRAPH Asia 2022)
              <br>
              <a href="https://mrtornado24.github.io/IDE-3D/">[Project]</a>
              <a href="https://arxiv.org/pdf/2205.15517.pdf">[PDF]</a>
	      <a href="https://github.com/MrTornado24/IDE-3D">[Code]</a>
              <a href="images/ide-3d.txt">[BibTeX]</a>
              <br>
              <p> We propose a high-resolution 3D-aware generative model that not only enables local control of the facial shape and texture, but also supports real-time, interactive editing. </p>
          </td>
      </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/cvpr22_fenerf.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>FENeRF: Face Editing in Neural Radiance Fields</papertitle>
                <br>
                <strong>Jingxiang Sun</strong>, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, Jue Wang
                <br>
                <em>2022 IEEE Conference on Computer Vision and Pattern Recognition</em>, CVPR 2022
                <br>
                <a href="https://mrtornado24.github.io/FENeRF/">[Project]</a>
                <a href="https://arxiv.org/pdf/2111.15490.pdf">[PDF]</a>
                <a href="https://github.com/MrTornado24/FENeRF">[Code]</a>
				<a href="images/fenerf.txt">[BibTeX]</a>
                <br>
                <p>We propose FENeRF, a 3D-aware generator that can produce view-consistent and locally-editable portrait images. Our method uses two decoupled latent codes to generate corresponding facial semantics and texture in a spatial aligned 3D volume with shared geometry. We also reveal that joint learning semantics and texture helps to generate finer geometry.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
		<video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                <source src='images/ijcv22_imocap.mp4'>
		</video>
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>iMoCap: Motion Capture from Internet Videos</papertitle>
                <br>
                Junting Dong*, Qing Shuai*, <strong>Jingxiang Sun</strong>, Yuanqing Zhang, Hujun Bao, Xiaowei Zhou (* equal contribution)
                <br>
                <em>2022 International Journal of Computer Vision </em>, IJCV 2022
                <br>
                <a href="https://link.springer.com/article/10.1007/s11263-022-01596-7">[PDF]</a>
				<a href="images/imocap.txt">[BibTeX]</a>
                <br>
                <p>We propose a novel optimization-based framework and experimentally demonstrate its ability to recover much more precise and detailed motion from multiple videos, compared against monocular pose estimation methods.</p>
            </td>
        </tr>

        <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one" >
                    <img src='images/icbda_bus.png' style="width:100%;max-width:100%; position: absolute;top: -5%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>BusTime: Which is the Right Prediction Model for My Bus Arrival Time?</papertitle>
                <br>
                Dairui Liu, <strong>Jingxiang Sun</strong>, Shen Wang
                <br>
                <em>2020 IEEE International Conference on Big Data Analytics</em>, ICBDA 2020
                <br>
                <a href="https://arxiv.org/pdf/2003.10373.pdf">[PDF]</a>
				        <a href="images/bus.txt">[BibTeX]</a>
                <br>
                <p>We propose a general and practical evaluation framework for analysing various widely used prediction models (i.e. delay, k- nearest-neighbor, kernel regression, additive model, and recur- rent neural network using long short term memory) for bus arrival time.</p>
            </td>
        </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Technical Reports</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr></tr>
<!--         <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
          <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                      <source src='images/dreamcraft3d.mp4'>
          </video>
            </div> -->
<!--         </td> -->
        <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>DeepSeek-VL: Towards Real-World Vision-Language Understanding</papertitle>
            <br>
            Haoyu Lu*, Wen Liu*, Bo Zhang**, Bingxuan Wang, Kai Dong, Bo Liu, <strong>Jingxiang Sun</strong>, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan (*Equal Contribution, **Project Lead)
            <br>
            <a href="https://huggingface.co/deepseek-ai">[Hugging Face]</a>
            <a href="https://arxiv.org/abs/2403.05525">[PDF]</a>
            <a href="https://github.com/deepseek-ai/DeepSeek-VL">[Code]</a>
            <br>
            <p> Introducing DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. </p>
        </td>
    </tr>


        <tr></tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://yudeng.github.io/">Yu Deng</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
